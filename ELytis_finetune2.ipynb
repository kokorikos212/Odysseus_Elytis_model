{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cpR7NYJHKve1rtaF5Z5PtjBg1ZJfgW1s",
      "authorship_tag": "ABX9TyOySHilio9aF55BILnyX6we",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kokorikos212/Odysseus_Elytis_model/blob/main/ELytis_finetune2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6ihYEsontCq",
        "outputId": "6129b63b-9990-4fb9-a0ad-e8884fdca45b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "VMdhQ9n6nzuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "lwrxH3hV7tAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qB_EBG-w261O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/poetry_segments\"\n",
        "# Read all text files from the folder\n",
        "texts = []\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            texts.append(file.read())\n",
        "\n",
        "# Create a DataFrame with the texts\n",
        "df = pd.DataFrame({'text': texts})\n",
        "dataset = Dataset.from_pandas(df)\n",
        "print(f\"Number of samples: {len(dataset)}\")"
      ],
      "metadata": {
        "id": "c3EFTgNe4y3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Set the pad_token to be the eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "PjE4C1gtC3No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=1024)\n",
        "\n",
        "# Apply the tokenizer to the dataset\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n"
      ],
      "metadata": {
        "id": "8iVbVTECC39f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset\n",
        "train_test_split = tokenized_dataset.train_test_split(test_size=0.1)  # Adjust test_size as needed\n",
        "\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']"
      ],
      "metadata": {
        "id": "AX37vpce4gis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.column_names"
      ],
      "metadata": {
        "id": "U81W8OCeBk-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "MQf7UYqCA3GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./elytis_gpt2\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "print(tokenized_dataset[0])\n"
      ],
      "metadata": {
        "id": "6LPtIX8FAYqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "YoolKXeYBMGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "AT0wU6awAebk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Compress the directory\n",
        "shutil.make_archive('/content/model_archive', 'zip', '/content/model_directory')\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/model_archive.zip')"
      ],
      "metadata": {
        "id": "S-AqjqnxXOJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare input text\n",
        "def preprocess_input(text, tokenizer):\n",
        "    return tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "text = \"Καλώς εχό\"\n",
        "inputs = preprocess_input(text, tokenizer)\n"
      ],
      "metadata": {
        "id": "loxIJAa2SAf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(inputs, model):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # Assuming the model is a classification model\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "    return predictions\n",
        "\n",
        "# Get model predictions\n",
        "predictions = get_predictions(inputs, model)\n",
        "print(predictions)\n",
        "\n"
      ],
      "metadata": {
        "id": "b_Z704DMBJkg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "445ad11b-3602-467d-aee3-ec337565bf3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  198,   138, 32830,   138,   234,   138,  7377,   118,   157,   234,\n",
            "         17394,   234, 35558]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "a Tensor with 13 elements cannot be converted to Scalar",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3511f13a4998>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Map predictions to labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"negative\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"positive\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 13 elements cannot be converted to Scalar"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for text generation\n",
        "max_length = 200  # Max length of the generated text\n",
        "temperature = 0.7  # Sampling temperature to control randomness\n",
        "top_k = 50  # Top-K sampling for more diverse generation\n",
        "top_p = 0.9  # Top-p sampling (nucleus sampling)"
      ],
      "metadata": {
        "id": "oVSHH0BGRjbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt, model, tokenizer, max_length=2000, temperature=0.7, top_k=50, top_p=0.9):\n",
        "    # Tokenize the input prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate text using the model\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
        "        do_sample=True,  # Enable sampling\n",
        "    )\n",
        "\n",
        "    # Decode the generated tokens into a human-readable string\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "NtP7fZ_4TOI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example prompt to generate Elytis-like text\n",
        "prompt = \"Έτσι μιλώ για σένα και για μένα\"\n",
        "\n",
        "# Generate and print the text\n",
        "elytis_like_text = generate_text(prompt, model, tokenizer, max_length, temperature, top_k, top_p)\n",
        "print(elytis_like_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "999x3zoHTQrx",
        "outputId": "c7b8a131-29f6-41ed-d125-ec4ec9da5b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Έτσι μιλώ για σένα και για μένα τοῦ πρόποτής διηταδυσαχείαν οὐτὰ εὼ λαρτος υπθξασάζοι ἐκΤβΛΡεΣΙσεἱ ηὦτι χγριον αὴ θεὶ Καττρεστες μοοᾶς τẓ Πολλαὸς ὅτων φνομισκύτψα\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_as_poetry(text):\n",
        "    lines = text.split('.')\n",
        "    formatted_poetry = '\\n'.join([line.strip() for line in lines if line.strip()])\n",
        "    return formatted_poetry\n",
        "\n",
        "# Format the generated text as poetry\n",
        "poetic_text = format_as_poetry(elytis_like_text)\n",
        "print(poetic_text)"
      ],
      "metadata": {
        "id": "5iPLziP_UJ-d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}