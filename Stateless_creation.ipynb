{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 7)\n",
    "from packaging import version\n",
    "import tensorflow as tf\n",
    "\n",
    "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")\n",
    "\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cleaned_combined_Elytis.txt\") as f:\n",
    "    data = f.read() \n",
    "\n",
    "def clean_text(text):\n",
    "    import re \n",
    "    \"\"\"\n",
    "    Removes all non-Greek characters and numbers from the given text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text to process.\n",
    "\n",
    "    Returns:\n",
    "    str: The cleaned text containing only Greek characters.\n",
    "    \"\"\"\n",
    "    # Define a regular expression pattern that matches Greek characters (uppercase and lowercase)\n",
    "    # and spaces (optional if you want to preserve spaces)\n",
    "    pattern = r'[^\\u0370-\\u03FF\\u1F00-\\u1FFF\\s]'\n",
    "    \n",
    "    # Use re.sub to replace all characters that don't match the pattern with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "data = clean_text(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ΐ', 'η', 'κ', 'ο', 'υ', 'δ', 'φ', 'α', 'λ', 'τ', 'σ', 'ς', 'ζ', 'ά', 'ξ', 'γ', 'ΰ', 'ν', ' ', 'β', 'ή', 'χ', 'ϋ', 'μ', 'π', 'ό', 'ε', 'ί', 'ρ', 'ψ', 'ω', 'θ', 'ι', 'ύ', 'ϊ', 'έ', 'ώ'}\n",
      "We should set the output layer to have 37 distinct characters.\n"
     ]
    }
   ],
   "source": [
    "print(set(data)) \n",
    "n_distinct_characters = len(set(data)) \n",
    "print(f\"We should set the output layer to have {n_distinct_characters} distinct characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll use a tf.keras.layers.TextVectorization layer to encode this text. We set split=\"character\" to get character-\n",
    "level encoding rather than the default word-level encoding, and we use\n",
    "standardize=\"lower\" to convert the text to lowercase (which will simplify the\n",
    "task):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 17:10:02.040716: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
    "standardize=\"lower\")\n",
    "text_vec_layer.adapt([data])\n",
    "encoded = text_vec_layer([data])[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each character is now mapped to an integer, starting at 2. The\n",
    "TextVectorization layer reserved the value 0 for padding tokens, and it\n",
    "reserved 1 for unknown characters. We won’t need either of these tokens fornow, so let’s subtract 2 from the character IDs and compute the number of\n",
    "distinct characters and the total number of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489572"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 37\n",
    "dataset_size = len(encoded) \n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can turn this very long sequence into\n",
    "a dataset of windows that we can then use to train a sequence-to-sequence\n",
    "RNN. The targets will be similar to the inputs, but shifted by one time step\n",
    "into the “future”.\n",
    "Let’s write a small utility function to convert a long sequence of character\n",
    "IDs into a dataset of input/target window pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow dataset from a sequence of tokens.\n",
    "    \n",
    "    Args:\n",
    "    sequence (list or np.array): The sequence of tokens.\n",
    "    length (int): Length of each sequence window.\n",
    "    shuffle (bool): Whether to shuffle the dataset.\n",
    "    seed (int): Random seed for shuffling.\n",
    "    batch_size (int): Batch size for training.\n",
    "    \n",
    "    Returns:\n",
    "    tf.data.Dataset: The TensorFlow dataset ready for training.\n",
    "    \"\"\"\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=100_000, seed=seed)\n",
    "    \n",
    "    ds = ds.map(lambda window: (window[:-1], window[1:]))  # Create (inputs, targets) pairs\n",
    "    ds = ds.batch(batch_size)  # Batch the data\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)  # Prefetch for performance\n",
    "    \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "train_set = to_dataset(encoded[:int(.9*len(encoded))], length=length, shuffle=True,\n",
    "seed=42)\n",
    "valid_set = to_dataset(encoded[int(.9*len(encoded)):int(.95*len(encoded))], length=length)\n",
    "test_set = to_dataset(encoded[int(.95*len(encoded)):], length=length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datasets():\n",
    "    # Save the train, validation, and test sets to separate directories\n",
    "    train_set.save(\"train_set_directory\")\n",
    "    valid_set.save(\"valid_set_directory\")\n",
    "    test_set.save(\"test_set_directory\")\n",
    "\n",
    "    print(\"Datasets exported successfully.\")\n",
    "# save_datasets() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_set, valid_set, n_tokens=37, embedding_dim=16, rnn_units=128, epochs=10):\n",
    "    \"\"\"\n",
    "    Function to train a text generation model using an RNN with early stopping.\n",
    "\n",
    "    Args:\n",
    "    train_set (tf.data.Dataset): The training dataset.\n",
    "    valid_set (tf.data.Dataset): The validation dataset.\n",
    "    n_tokens (int): Number of distinct characters in the dataset.\n",
    "    embedding_dim (int): Dimension of the embedding layer.\n",
    "    rnn_units (int): Number of units in the RNN layer.\n",
    "    epochs (int): Number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "    model (tf.keras.Model): The trained Keras model.\n",
    "    history (History object): The training history object.\n",
    "    \"\"\"\n",
    "    # Create the model architecture\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=embedding_dim),\n",
    "        tf.keras.layers.GRU(rnn_units, return_sequences=True),\n",
    "        tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Define the checkpoint callback to save the best model based on validation accuracy\n",
    "    model_ckpt = tf.keras.callbacks.ModelCheckpoint(\"my_ELytis_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "\n",
    "    # Define the early stopping callback\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",  # Monitor validation loss\n",
    "        patience=3,          # Stop if no improvement for 3 epochs\n",
    "        restore_best_weights=True  # Restore model weights from the epoch with the best validation loss\n",
    "    )\n",
    "\n",
    "    # Train the model with both callbacks\n",
    "    history = model.fit(train_set, validation_data=valid_set, epochs=epochs, callbacks=[model_ckpt, early_stopping])\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# Call the function with the updated output layer size\n",
    "model, history = train_model(train_set, valid_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Elytis_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2), # no <PAD> or <UNK> tokens\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the saved directory\n",
    "Elytis_model = tf.keras.models.load_model('my_ELytis_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted next character: σ\n"
     ]
    }
   ],
   "source": [
    "# Example input text\n",
    "input_text = \"πρέπ\"  # Example Greek seed text\n",
    "\n",
    "# Preprocess the input text using the text vectorization layer\n",
    "# Ensure the input is a batch (list) of strings\n",
    "input_data = text_vec_layer([input_text])\n",
    "\n",
    "# Predict the next character's probability distribution\n",
    "y_proba = Elytis_model.predict(input_data)[0, -1]  # Get probabilities for the last character\n",
    "\n",
    "# Choose the most probable character ID\n",
    "y_pred = tf.argmax(y_proba).numpy()  # Convert to a NumPy integer\n",
    "\n",
    "# Map the predicted character ID back to the actual character\n",
    "char = text_vec_layer.get_vocabulary()[y_pred + 2]  # Adjust index offset if necessary\n",
    "\n",
    "print(\"Predicted next character:\", char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 1, 0, 2, 1, 0, 0, 1]])>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]]) # probas = 50%, 40%, and 10%\n",
    "tf.random.set_seed(42)\n",
    "tf.random.categorical(log_probas, num_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text):\n",
    "    \"\"\"\n",
    "    Predicts the next character for the given text using the trained model.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text to predict the next character for.\n",
    "\n",
    "    Returns:\n",
    "    str: The next predicted character.\n",
    "    \"\"\"\n",
    "\n",
    "    input_data = tf.constant([text])  # Shape (1,)\n",
    "\n",
    "    # Vectorize the input using the text vectorization layer\n",
    "    vectorized_input = text_vec_layer(input_data)  # Shape (1, sequence_length)\n",
    "\n",
    "    # Predict the next character's probability distribution\n",
    "    y_proba = Elytis_model.predict(vectorized_input)[0, -1]  # Get probabilities for the last character\n",
    "\n",
    "    # Choose the most probable character ID\n",
    "    y_pred = tf.argmax(y_proba).numpy()  # Get the ID of the most probable character\n",
    "\n",
    "    # Convert character ID back to character\n",
    "    return text_vec_layer.get_vocabulary()[y_pred + 2]  # Offset for any special tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature):\n",
    "    \"\"\"\n",
    "    Generates the next character in the sequence based on the model's prediction and the given temperature.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text used as the seed for generating the next character.\n",
    "    temperature (float): The temperature value used to control the randomness of the predictions.\n",
    "\n",
    "    Returns:\n",
    "    str: The next predicted character.\n",
    "    \"\"\"\n",
    "    # Preprocess the input text to vector form\n",
    "    input_data = text_vec_layer([text])  # Assuming text_vec_layer is the preprocessing layer\n",
    "\n",
    "    # Predict the next character's probability distribution\n",
    "    y_proba = Elytis_model.predict(input_data)[0, -1, :]  # Get probabilities for the last character\n",
    "\n",
    "    # Apply temperature scaling to logits\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "\n",
    "    # Sample the next character using tf.random.categorical\n",
    "    char_id = tf.random.categorical(rescaled_logits[None, :], num_samples=1)[0, 0].numpy()\n",
    "\n",
    "    # Convert character ID back to the corresponding character\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(text, temperature=0.1, n_chars=500):\n",
    "    \"\"\"\n",
    "    Extends the input text by generating new characters using the trained model.\n",
    "\n",
    "    Args:\n",
    "    text (str): The initial text to extend.\n",
    "    n_chars (int): The number of characters to generate.\n",
    "\n",
    "    Returns:\n",
    "    str: The extended text after generating new characters.\n",
    "    \"\"\"\n",
    "    for _ in range(n_chars):\n",
    "        next_character = next_char(text,temperature)\n",
    "        text += next_character  # Append the generated character to the text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"Κάπου εδώ πρέπει\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
